{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the Analyzers, Tokenizers, and Filters metapy tutorial. First, you should read the following two MeTA tutorials:\n",
    "- [MeTA System Overview](https://meta-toolkit.org/overview-tutorial.html). Everything on this page is relevant to metapy except for the *Unit tests* section (you can't run them in Python).\n",
    "- [Analyzers, Tokenizers, and Filters](https://meta-toolkit.org/analyzers-filters-tutorial.html). Everything on this page is relevant except for the *Extending MeTA With Your Own Filters* section.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "First, let's create a document to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import metapy\n",
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can make our own filter chain and run it on the document's content. Let's start with a simple example of only using `ICUTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'I',\n",
       " 'said',\n",
       " 'that',\n",
       " 'I',\n",
       " \"can't\",\n",
       " 'believe',\n",
       " 'that',\n",
       " 'it',\n",
       " 'only',\n",
       " 'costs',\n",
       " '$',\n",
       " '19.95',\n",
       " '!',\n",
       " '</s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer()\n",
    "tok.set_content(doc.content())\n",
    "[t for t in tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "See how the begin and end sentence markers (`<s>` and `</s>`) are inserted at the beginning and end of each sentence. We get an ordered list from using a tokenizer or filter.\n",
    "\n",
    "Next, use `LowercaseFilter` to convert each token to lowercase. We use the previous `tok` (which is an `ICUTokenizer`) in the constructor to `LowercaseFilter`. This lets us connect an arbitrary amount of filters together with a tokenizer at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'i',\n",
       " 'said',\n",
       " 'that',\n",
       " 'i',\n",
       " \"can't\",\n",
       " 'believe',\n",
       " 'that',\n",
       " 'it',\n",
       " 'only',\n",
       " 'costs',\n",
       " '$',\n",
       " '19.95',\n",
       " '!',\n",
       " '</s>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer()\n",
    "tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "tok.set_content(doc.content())\n",
    "[t for t in tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just like in MeTA, metapy's filter chain can be created from a config file. Create the following file called `config.toml`. It will perform the same tokenization and filtering as above (`ICUTokenizer -> LowercaseFilter`). Then, it will aggregate token counts together using an *n*-gram words analyzer.\n",
    "\n",
    "```toml\n",
    "[[analyzers]]\n",
    "method = \"ngram-word\"\n",
    "ngram = 1\n",
    "filter = [{type = \"icu-tokenizer\"}, {type = \"lowercase\"}]\n",
    "```\n",
    "\n",
    "Now, you can load this config file to create a unigram words analyzer. This uses the specified tokenizer/filter chain and analyzer type to convert a document into a dictionary of features and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 1,\n",
       " '$': 1,\n",
       " '19.95': 1,\n",
       " '</s>': 1,\n",
       " '<s>': 1,\n",
       " 'believe': 1,\n",
       " \"can't\": 1,\n",
       " 'costs': 1,\n",
       " 'i': 2,\n",
       " 'it': 1,\n",
       " 'only': 1,\n",
       " 'said': 1,\n",
       " 'that': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = metapy.analyzers.load('config.toml')\n",
    "ana.analyze(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The tokens *i* and *that* are shown with two counts, while all the other tokens have 1 count. These features can then be passed to other parts of metapy, such as ranking functions or indexers.\n",
    "\n",
    "We can also manually specify the analyzer instead of loading it from the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 1,\n",
       " '$': 1,\n",
       " '19.95': 1,\n",
       " '</s>': 1,\n",
       " '<s>': 1,\n",
       " 'believe': 1,\n",
       " \"can't\": 1,\n",
       " 'costs': 1,\n",
       " 'i': 2,\n",
       " 'it': 1,\n",
       " 'only': 1,\n",
       " 'said': 1,\n",
       " 'that': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
    "ana.analyze(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('$', '19.95', '!'): 1,\n",
       " ('19.95', '!', '</s>'): 1,\n",
       " ('<s>', 'i', 'said'): 1,\n",
       " ('believe', 'that', 'it'): 1,\n",
       " (\"can't\", 'believe', 'that'): 1,\n",
       " ('costs', '$', '19.95'): 1,\n",
       " ('i', \"can't\", 'believe'): 1,\n",
       " ('i', 'said', 'that'): 1,\n",
       " ('it', 'only', 'costs'): 1,\n",
       " ('only', 'costs', '$'): 1,\n",
       " ('said', 'that', 'i'): 1,\n",
       " ('that', 'i', \"can't\"): 1,\n",
       " ('that', 'it', 'only'): 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(3, tok)\n",
    "ana.analyze(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Usually, metapy applications will create and call analyzers based on a config file, so you won't have to create your own manually. However, it may still be useful if you are performing your own analysis that is not part of MeTA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
