{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the Python bindings, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metapy.__version__ # you will want your version to be >= to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to, you can inform MeTA to output log data to stderr like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metapy.log_to_stderr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download a list of stopwords and a sample dataset to begin exploring MeTA's topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-31 17:19:53--  https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving raw.githubusercontent.com... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2747 (2.7K) [text/plain]\n",
      "Saving to: ‘lemur-stopwords.txt’\n",
      "\n",
      "lemur-stopwords.txt 100%[===================>]   2.68K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2017-07-31 17:19:54 (47.6 MB/s) - ‘lemur-stopwords.txt’ saved [2747/2747]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-31 17:19:54--  https://meta-toolkit.org/data/2016-01-26/ceeaus.tar.gz\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving meta-toolkit.org... 50.116.41.177, 2600:3c02::f03c:91ff:feae:b777\n",
      "Connecting to meta-toolkit.org|50.116.41.177|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘ceeaus.tar.gz’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://meta-toolkit.org/data/2016-01-26/ceeaus.tar.gz\n",
    "!tar xf ceeaus.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to index our data to proceed. We eventually want to be able to extract the bag-of-words representation for our individual documents, so we will want a `ForwardIndex` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fidx = metapy.index.make_forward_index('[ceeaus-config.toml](https://github.com/meta-toolkit/metapy/blob/master/tutorials/ceeaus-config.toml)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in classification, the feature set used for the topic modeling will be the feature set used at the time of indexing, so if you want to play with a different set of features (like bigram words), you will need to re-index your data.\n",
    "\n",
    "For now, we've just stuck with the default filter chain for unigram words, so we're operating in the traditional bag-of-words space.\n",
    "\n",
    "Let's load our documents into memory to run the topic model inference now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = metapy.learn.Dataset(fidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to find some topics for this dataset. To do so, we're going to use a generative model called a topic model.\n",
    "\n",
    "There are many different topic models in the literature, but the most commonly used topic model is Latent Dirichlet Allocation. Here, we propose that there are K topics (represented with a categorical distribution over words) $\\phi_k$ from which all of our documents are genereated. These K topics are modeled as being sampled from a Dirichlet distribution with parameter $\\vec{\\alpha}$. Then, to generate a document $d$, we first sample a distribution over the K topics $\\theta_d$ from another Dirichlet distribution with parameter $\\vec{\\beta}$. Then, for each word in this document, we first sample a topic identifier $z \\sim \\theta_d$ and then the word by drawing from the topic we selected ($w \\sim \\phi_z$). Refer to the [Wikipedia article on LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) for more information.\n",
    "\n",
    "The goal of running inference for an LDA model is to infer the latent variables $\\phi_k$ and $\\theta_d$ for all of the $K$ topics and $D$ documents, respectively. MeTA provides a number of different inference algorithms for LDA, as each one entails a different set of trade-offs (inference in LDA is intractable, so all inference algorithms are approximations; different algorithms entail different approximation guarantees, running times, and required memroy consumption). For now, let's run a Variational Infernce algorithm called CVB0 to find two topics. (In practice you will likely be finding many more topics than just two, but this is a very small toy dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_inf = metapy.topics.LDACollapsedVB(dset, num_topics=2, alpha=1.0, beta=0.01)\n",
    "lda_inf.run(num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above ran the CVB0 algorithm for 1000 iterations, or until an algorithm-specific convergence criterion was met. Now let's save the current estimate for our topics and topic proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inf.save('lda-cvb0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interrogate the topic inference results by using the `TopicModel` query class. Let's load our inference results back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = metapy.topics.TopicModel('lda-cvb0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at our topics. A typical way of doing this is to print the top $k$ words in each topic, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3341, 0.13110421021038324),\n",
       " (3045, 0.05434941973171466),\n",
       " (2677, 0.03678014124195563),\n",
       " (3346, 0.03349270116563351),\n",
       " (281, 0.0225307149144554),\n",
       " (3729, 0.015620504332737565),\n",
       " (1953, 0.012780941699372448),\n",
       " (707, 0.012635092781408291),\n",
       " (592, 0.011987205216970597),\n",
       " (2448, 0.011317761088081552)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.top_k(tid=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models operate on term ids instead of raw text strings, so let's convert this to a human readable format by using the vocabulary contained in our `ForwardIndex` to map the term ids to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 0.13110421021038324),\n",
       " ('restaur', 0.05434941973171466),\n",
       " ('peopl', 0.03678014124195563),\n",
       " ('smoker', 0.03349270116563351),\n",
       " ('ban', 0.0225307149144554),\n",
       " ('think', 0.015620504332737565),\n",
       " ('japan', 0.012780941699372448),\n",
       " ('complet', 0.012635092781408291),\n",
       " ('cigarett', 0.011987205216970597),\n",
       " ('non', 0.011317761088081552)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 0.06705633566728429),\n",
       " ('job', 0.056059202698078466),\n",
       " ('part', 0.052222972425643444),\n",
       " ('student', 0.04642930410399952),\n",
       " ('colleg', 0.03488134676327594),\n",
       " ('work', 0.029067430777979537),\n",
       " ('money', 0.02885016964119207),\n",
       " ('think', 0.022331317888389407),\n",
       " ('import', 0.02075566243543525),\n",
       " ('studi', 0.015483008722512084)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pretty clearly see that this particular dataset was about two major issues: smoking in public and part time jobs for students. This dataset is actually a collection of essays written by students, and there just so happen to be two different topics they can choose from!\n",
    "\n",
    "The topics are pretty clear in this case, but in some cases it is also useful to score the terms in a topic using some function of the probability of the word in the topic and the probability of the word in the other topics. Intuitively, we might want to select words from each topic that best reflect that topic's content by picking words that both have high probability in that topic **and** have low probability in the other topics. In other words, we want to balance between high probability terms and highly specific terms (this is kind of like a tf-idf weighting). One such scoring function is provided by the toolkit in `BLTermScorer`, which implements a scoring function proposed by Blei and Lafferty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smoke', 0.8741661352396957),\n",
       " ('restaur', 0.3174611854294489),\n",
       " ('smoker', 0.20060321654683078),\n",
       " ('ban', 0.12853060169345398),\n",
       " ('cigarett', 0.06557618816704736),\n",
       " ('non', 0.06128436226482625),\n",
       " ('complet', 0.061053888089489305),\n",
       " ('japan', 0.05846310193983406),\n",
       " ('health', 0.050548462085096446),\n",
       " ('seat', 0.04533997725235284)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = metapy.topics.BLTermScorer(model)\n",
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=0, scorer=scorer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('job', 0.348219801380548),\n",
       " ('part', 0.3131101849492852),\n",
       " ('student', 0.283288839381727),\n",
       " ('colleg', 0.20808951747041338),\n",
       " ('time', 0.17797627851908793),\n",
       " ('money', 0.16234654268837612),\n",
       " ('work', 0.15585386268480006),\n",
       " ('studi', 0.08228275310960287),\n",
       " ('learn', 0.06491893059362819),\n",
       " ('experi', 0.05494520825302107)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(fidx.term_text(pr[0]), pr[1]) for pr in model.top_k(tid=1, scorer=scorer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the uninformative word stem \"think\" was downweighted from the word list from each topic, since it had relatively high probability in either topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the inferred topic distribution for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metapy.stats.Multinomial {0: 0.021341, 1: 0.978659}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_distribution(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our first document was written by a student who chose the part-time job essay topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metapy.stats.Multinomial {0: 0.978797, 1: 0.021203}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_distribution(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...whereas this document looks like it was written by a student who chose the public smoking essay topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
